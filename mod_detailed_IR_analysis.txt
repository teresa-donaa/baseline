Number of models:																				
1																				
Number of cores																				
25																				
Number of games																				
1000																				
Iterations per year																				
25000																				
Maximum number of years																				
50000																				
"Performance measurement period time (Y, Y <= 0 : at convergence after mandatory training of (-Y) years; X >= 1 : starts right after the completion of year X-1):"																				
0																				
Performance measurement period length: 																				
4																				
Number of Agents:																				
2																				
Depth of State:																				
1																				
Number of Prices:																				
15																				
Type of Exploration Mechanism (1 = constant epsilon greedy; 2 = exponentially decreasing epsilon (beta); 3 = exponentially decreasing epsilon (m))																				
2																				
Type of Payoff Input (0 = PI1 matrix given below in the states natural order; 1 = Singh & Vives demand; 2 = Logit demand; 3 = Logit demand with zero sigma (perfect competition))																				
2																				
"Q matrix initialization (a numAgents array; R = Random uniform in (lb,ub); U = Uniform at u; T = Trained (n); O = randomizing over opponents decision) (Check the code for QMatrixInitializationT to see whether additional lines need to be added below)"																				
O	O																			
Compute Q-Learning results (0 = No; 1 = Yes)																				
0																				
Compute results at convergence (prices and profits) (0 : No; 1 = Yes)																				
0																				
Compute pre-shock cycles (prices and profits) (0 : No; 1 = Yes)																				
0																				
Compute Impulse Response analysis with a one-period deviation to static Best Response (0 = No; 1 = Yes)																				
0																				
Compute Impulse Response analysis with a temporary or permanent deviation to Nash (0 = No; -1 = Permanent; X > 1 = Duration of temporary shock)																				
0																				
Compute Impulse Response analysis with a one-period deviation to all prices (0 = No; 1 = Yes)																				
0																				
Compute Detailed Impulse Response analysis with a one-period deviation to all prices (0 = No; 1 = Yes)																				
1																				
Compute Equilibrium Check (0 = NO; 1 = Yes)																				
0																				
Compute Q Gap w.r.t. Maximum (0 = NO; 1 = YES)																				
0																				
Compute Average PI Gap w.r.t. Maximum (0 = NO; 1 = YES)																				
0																				
"Compute restart results (0 = NO; X > 1 = Number of agents (the last X) who restart learning; the others start from the learned Q, do not experiment but can learn)"																				
0																				
Compute mixed strategies results 																				
0	0																			
Model	PrintQ	PrintP	Alpha1	Alpha2	Beta1	Beta2	Delta1	Delta2	a0	a1	a2	c1	c2	sigma	extend1	extend2	NashP1	NashP2	CoopP1	CoopP2
1	0	0	0.15	0.15	0.1	0.1	0.95	0.95	0	2	2	1	1	0.25	0.1	0.1	1.47293	1.47293	1.92498	1.92498
